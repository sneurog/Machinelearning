{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b102140-802a-49e7-8c9d-c05e589e13a5",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Homework assignment #10: Recurrent Neural Network, LSTM </h1>\n",
    "<h2 align=\"center\"> Assigned Nov 10 and Due Nov 21 </h2>\n",
    "<h3 align=\"center\"> MSSE 277B: Machine Learning Algorithms </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7207cda-1e05-499c-a345-b19ae48c6482",
   "metadata": {},
   "source": [
    "1. **LSTM applied to SMILES string generation. (10 pt)** Using the SMILES string from the ANI\n",
    "dataset with upto 6 heavy atoms, build a LSTM generative model that can generate new smiles string\n",
    "with given initial character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5217cd13-0d17-4afc-94db-fa65349c01a7",
   "metadata": {},
   "source": [
    "**(a) (3pt)** Process the smiles strings from ANI dataset by adding a starting character at the beginning\n",
    "and an ending character at the end. Look over the dataset and define the vocabulary, use one hot\n",
    "encoding to encode your smiles strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ff00c9-8831-48b1-9b24-b023ea8a49ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730ddf61-01e7-4275-a747-bf92d006559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'H',\n",
       "  ']',\n",
       "  'C',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']'],\n",
       " ['[', 'H', ']', 'N', '(', '[', 'H', ']', ')', '[', 'H', ']'],\n",
       " ['[', 'H', ']', 'O', '[', 'H', ']'],\n",
       " ['[',\n",
       "  'H',\n",
       "  ']',\n",
       "  'C',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  'C',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']'],\n",
       " ['[',\n",
       "  'H',\n",
       "  ']',\n",
       "  'N',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  'C',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  '(',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']',\n",
       "  ')',\n",
       "  '[',\n",
       "  'H',\n",
       "  ']']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the provided dataset\n",
    "file_path = 'ani_smiles.pkl'\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(file_path)\n",
    "dataset[:5]  # Display the first few entries for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db78f4e6-4088-4fa1-b437-3127d6bf4d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[[H]C([H])([H])[H]$',\n",
       "  '[[H]N([H])[H]$',\n",
       "  '[[H]O[H]$',\n",
       "  '[[H]C([H])([H])C([H])([H])[H]$',\n",
       "  '[[H]N([H])C([H])([H])[H]$'],\n",
       " ['#',\n",
       "  '$',\n",
       "  '(',\n",
       "  ')',\n",
       "  '1',\n",
       "  '2',\n",
       "  '=',\n",
       "  'C',\n",
       "  'H',\n",
       "  'N',\n",
       "  'O',\n",
       "  '[',\n",
       "  ']',\n",
       "  'c',\n",
       "  'n',\n",
       "  'o'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Concatenate characters in each SMILES string\n",
    "concatenated_smiles = [''.join(smiles) for smiles in dataset]\n",
    "\n",
    "# Step 2: Add starting '^' and ending '$' characters to each SMILES string\n",
    "processed_smiles = ['[' + smiles + '$' for smiles in concatenated_smiles]\n",
    "\n",
    "# Step 3: Define the vocabulary (unique characters in the dataset)\n",
    "unique_chars = set(''.join(processed_smiles))\n",
    "vocabulary = sorted(list(unique_chars))  # Sort for consistency\n",
    "\n",
    "# Display first few processed SMILES and the vocabulary\n",
    "processed_smiles[:5], vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56da09ba-2cba-48b6-a095-8264ac4f90de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " '[[H]C([H])([H])[H]$')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: One-hot Encode the SMILES Strings\n",
    "\n",
    "# Create a mapping from characters to indices and vice versa\n",
    "char_to_index = {char: i for i, char in enumerate(vocabulary)}\n",
    "index_to_char = {i: char for char, i in char_to_index.items()}\n",
    "\n",
    "# Function to one-hot encode a SMILES string\n",
    "def one_hot_encode(smiles, char_to_index, max_length):\n",
    "    # Initialize a matrix of zeros with dimensions (max_length, len(vocabulary))\n",
    "    encoding = np.zeros((max_length, len(char_to_index)), dtype=np.float32)\n",
    "\n",
    "    # Encode each character in the SMILES string\n",
    "    for i, char in enumerate(smiles):\n",
    "        encoding[i, char_to_index[char]] = 1.0\n",
    "    return encoding\n",
    "\n",
    "# Determine the maximum length of SMILES strings for consistent encoding\n",
    "max_smiles_length = max(len(s) for s in processed_smiles)\n",
    "\n",
    "# One-hot encode the SMILES strings\n",
    "encoded_smiles = [one_hot_encode(smiles, char_to_index, max_smiles_length) for smiles in processed_smiles]\n",
    "\n",
    "# Example: Display the encoding of the first SMILES string\n",
    "encoded_smiles[0], processed_smiles[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747a1d17-f111-4ad3-816b-c0c60900a596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " '[[H]C([H])([H])[H]$')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out SMILES strings longer than the threshold\n",
    "filtered_smiles = [s for s in processed_smiles if len(s) > 1 ]\n",
    "\n",
    "# Update the maximum SMILES length based on the filtered dataset\n",
    "max_smiles_length = max(len(s) for s in filtered_smiles)\n",
    "\n",
    "# One-hot encode the filtered SMILES strings\n",
    "encoded_smiles = [one_hot_encode(smiles, char_to_index, max_smiles_length) for smiles in filtered_smiles]\n",
    "\n",
    "# Example: Display the encoding of the first SMILES string in the filtered dataset\n",
    "encoded_smiles[0], filtered_smiles[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f7de6-3075-4f50-97e1-efc46d79caff",
   "metadata": {},
   "source": [
    "**(b) (7pt)** Build a LSTM model with 1 recurrent layer. Starting with the starting character and grow\n",
    "a string character by character using model prediction until it reaches a ending character. Look\n",
    "at the string you grown, is it a valid SMILES string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd2fce5-a603-452f-ab87-747527099830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SMILESLSTM(\n",
       "  (lstm): LSTM(16, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the LSTM model\n",
    "class SMILESLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size, n_layers=1):\n",
    "        super(SMILESLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(vocabulary)  # Size of the one-hot encoded vectors\n",
    "hidden_dim = 128  # Number of features in the hidden state\n",
    "output_size = len(vocabulary)  # Size of the output (same as vocabulary size)\n",
    "\n",
    "# Create the LSTM model\n",
    "lstm_model = SMILESLSTM(input_size, hidden_dim, output_size)\n",
    "\n",
    "# Display the model summary\n",
    "lstm_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e8fb790-ddf5-4429-a8d5-530d7b711834",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_smiles_array = np.array(encoded_smiles)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training (80%) and validation (20%) sets\n",
    "train_data, val_data = train_test_split(encoded_smiles_array, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ec4df0-4663-4a00-afe3-1c42ea81a38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/4cs993fx7s3_5hvg665c2hlr0000gn/T/ipykernel_11487/3439695489.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  input_seq = torch.tensor([s[:-1] for s in batch], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 0.038311253917419304, Validation Loss: 0.03180019868969244\n",
      "Epoch 2/50, Training Loss: 0.028995955226111546, Validation Loss: 0.029307341171523272\n",
      "Epoch 3/50, Training Loss: 0.025205014115673, Validation Loss: 0.025906890125597937\n",
      "Epoch 4/50, Training Loss: 0.02236939899328738, Validation Loss: 0.02250235801362722\n",
      "Epoch 5/50, Training Loss: 0.021146081766839755, Validation Loss: 0.021307654636727888\n",
      "Epoch 6/50, Training Loss: 0.019124165261532627, Validation Loss: 0.01872154851417757\n",
      "Epoch 7/50, Training Loss: 0.016204854526088736, Validation Loss: 0.015651222002708305\n",
      "Epoch 8/50, Training Loss: 0.013310098547046468, Validation Loss: 0.012590631758425869\n",
      "Epoch 9/50, Training Loss: 0.011028224381349854, Validation Loss: 0.010738335423550363\n",
      "Epoch 10/50, Training Loss: 0.009678573594928462, Validation Loss: 0.0094645068807117\n",
      "Epoch 11/50, Training Loss: 0.008739324949555477, Validation Loss: 0.008642628368011301\n",
      "Epoch 12/50, Training Loss: 0.010616025198145775, Validation Loss: 0.010491254976240256\n",
      "Epoch 13/50, Training Loss: 0.00899348231190342, Validation Loss: 0.00886011696131216\n",
      "Epoch 14/50, Training Loss: 0.008072058970140199, Validation Loss: 0.008170671887316946\n",
      "Epoch 15/50, Training Loss: 0.007546321462608327, Validation Loss: 0.007690446272408221\n",
      "Epoch 16/50, Training Loss: 0.007170663609848184, Validation Loss: 0.007321409670646581\n",
      "Epoch 17/50, Training Loss: 0.006865208221357421, Validation Loss: 0.007008560633255264\n",
      "Epoch 18/50, Training Loss: 0.006547201530071302, Validation Loss: 0.006986485783663173\n",
      "Epoch 19/50, Training Loss: 0.006479231903781998, Validation Loss: 0.006611531652973197\n",
      "Epoch 20/50, Training Loss: 0.006108436059985457, Validation Loss: 0.00621979957246511\n",
      "Epoch 21/50, Training Loss: 0.005815330913848123, Validation Loss: 0.006103734343738879\n",
      "Epoch 22/50, Training Loss: 0.0056073360439747744, Validation Loss: 0.0059201509433951085\n",
      "Epoch 23/50, Training Loss: 0.005420416782973177, Validation Loss: 0.005699476027219309\n",
      "Epoch 24/50, Training Loss: 0.0052333601492609684, Validation Loss: 0.0054840729229867794\n",
      "Epoch 25/50, Training Loss: 0.00507517851426103, Validation Loss: 0.005261761404700198\n",
      "Epoch 26/50, Training Loss: 0.004897724079378581, Validation Loss: 0.005177876117539271\n",
      "Epoch 27/50, Training Loss: 0.004749223568850318, Validation Loss: 0.0049093454065969435\n",
      "Epoch 28/50, Training Loss: 0.00461139014295939, Validation Loss: 0.004794693262563587\n",
      "Epoch 29/50, Training Loss: 0.004459667277369795, Validation Loss: 0.004639105439859595\n",
      "Epoch 30/50, Training Loss: 0.0043527719875176745, Validation Loss: 0.004634522701387351\n",
      "Epoch 31/50, Training Loss: 0.0042270154897439275, Validation Loss: 0.004365172594954065\n",
      "Epoch 32/50, Training Loss: 0.004094245954085205, Validation Loss: 0.004394144324933068\n",
      "Epoch 33/50, Training Loss: 0.0040450574630229484, Validation Loss: 0.004166032803260674\n",
      "Epoch 34/50, Training Loss: 0.0039287016406066, Validation Loss: 0.004274786192145052\n",
      "Epoch 35/50, Training Loss: 0.003827827409078172, Validation Loss: 0.004044147155715921\n",
      "Epoch 36/50, Training Loss: 0.003727985598609946, Validation Loss: 0.0038761151796680385\n",
      "Epoch 37/50, Training Loss: 0.0036398442514703772, Validation Loss: 0.0038194010655085244\n",
      "Epoch 38/50, Training Loss: 0.0035651458458880246, Validation Loss: 0.0037988642767324285\n",
      "Epoch 39/50, Training Loss: 0.0035816067461408464, Validation Loss: 0.0037916556084896884\n",
      "Epoch 40/50, Training Loss: 0.003462713906320475, Validation Loss: 0.0036092697732192647\n",
      "Epoch 41/50, Training Loss: 0.003387755424962879, Validation Loss: 0.003517840541688736\n",
      "Epoch 42/50, Training Loss: 0.003329342189060766, Validation Loss: 0.003460133606094425\n",
      "Epoch 43/50, Training Loss: 0.003281990918567625, Validation Loss: 0.0034079794998222827\n",
      "Epoch 44/50, Training Loss: 0.0032459840199536524, Validation Loss: 0.003441545880783749\n",
      "Epoch 45/50, Training Loss: 0.0032609351884342183, Validation Loss: 0.003434579082801517\n",
      "Epoch 46/50, Training Loss: 0.00314944148021566, Validation Loss: 0.003353702609485152\n",
      "Epoch 47/50, Training Loss: 0.0030944968052839827, Validation Loss: 0.003291367853091935\n",
      "Epoch 48/50, Training Loss: 0.003062962033485962, Validation Loss: 0.0032647210831022533\n",
      "Epoch 49/50, Training Loss: 0.003056598174790878, Validation Loss: 0.0032021357851513364\n",
      "Epoch 50/50, Training Loss: 0.0030290451174401966, Validation Loss: 0.0032019543193154416\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50  # Number of epochs for training\n",
    "batch_size = 64  # Batch size for training\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training and Validation Loop\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()  # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Training Phase\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        batch = train_data[i:i+batch_size]\n",
    "        input_seq = torch.tensor([s[:-1] for s in batch], dtype=torch.float32)\n",
    "\n",
    "        # Extracting the indices of the 'hot' elements for the target sequence\n",
    "        target_seq = torch.tensor([np.argmax(s, axis=1) for s in batch], dtype=torch.long)\n",
    "        target_seq = target_seq[:, 1:].contiguous().view(-1)  # Exclude the first character and flatten\n",
    "\n",
    "        hidden = lstm_model.init_hidden(len(batch))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _ = lstm_model(input_seq, hidden)\n",
    "        loss = criterion(output.view(-1, len(vocabulary)), target_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation Phase\n",
    "    lstm_model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_data), batch_size):\n",
    "            batch = val_data[i:i+batch_size]\n",
    "            input_seq = torch.tensor([s[:-1] for s in batch], dtype=torch.float32)\n",
    "            \n",
    "            # Extracting the indices of the 'hot' elements for the target sequence\n",
    "            target_seq = torch.tensor([np.argmax(s, axis=1) for s in batch], dtype=torch.long)\n",
    "            target_seq = target_seq[:, 1:].contiguous().view(-1)  # Exclude the first character and flatten\n",
    "\n",
    "            hidden = lstm_model.init_hidden(len(batch))\n",
    "            output, _ = lstm_model(input_seq, hidden)\n",
    "            loss = criterion(output.view(-1, len(vocabulary)), target_seq)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_data)\n",
    "    avg_val_loss = val_loss / len(val_data)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "884b105c-b3b4-4062-bb4c-a31d0ccb9015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SMILES: [[H]OC([H])([H])C1([H])C([H])([H])[H]\n"
     ]
    }
   ],
   "source": [
    "def generate_smiles(model, start_token='[', max_length=100):\n",
    "    model.eval()  # Switch model to evaluation mode\n",
    "    initial_input = one_hot_encode(start_token, char_to_index, 1)\n",
    "    initial_input = torch.tensor(initial_input).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    # Generate SMILES\n",
    "    smiles = start_token\n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model(initial_input, hidden)\n",
    "        probabilities = F.softmax(output[0, -1], dim=0).detach().numpy()\n",
    "        next_char_index = np.random.choice(len(vocabulary), p=probabilities)\n",
    "        next_char = index_to_char[next_char_index]\n",
    "\n",
    "        if next_char == '$':  # End token\n",
    "            break\n",
    "\n",
    "        smiles += next_char\n",
    "        initial_input = one_hot_encode(next_char, char_to_index, 1)\n",
    "        initial_input = torch.tensor(initial_input).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    return smiles\n",
    "\n",
    "# Generate a new SMILES string\n",
    "new_smiles = generate_smiles(lstm_model)\n",
    "print(\"Generated SMILES:\", new_smiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb7818-f1c3-4d0f-aa4c-297d05528944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem277b",
   "language": "python",
   "name": "chem277b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
